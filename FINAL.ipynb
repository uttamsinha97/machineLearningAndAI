{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thatåÕs the way u feel. ThatåÕs the wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5   spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    ham  Even my brother is not like to speak with me. ...\n",
       "7    ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8   spam  WINNER!! As a valued network customer you have...\n",
       "9   spam  Had your mobile 11 months or more? U R entitle...\n",
       "10   ham  I'm gonna be home soon and i don't want to tal...\n",
       "11  spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "12  spam  URGENT! You have won a 1 week FREE membership ...\n",
       "13   ham  I've been searching for the right words to tha...\n",
       "14   ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "15  spam  XXXMobileMovieClub: To use your credit, click ...\n",
       "16   ham                         Oh k...i'm watching here:)\n",
       "17   ham  Eh u remember how 2 spell his name... Yes i di...\n",
       "18   ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...\n",
       "19  spam  England v Macedonia - dont miss the goals/team..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'C:\\\\Users\\\\UttamSinha\\\\spam.csv'\n",
    "messages = pd.read_csv(url,encoding='latin-1')\n",
    "messages = messages.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "messages= messages.rename(columns={\"v1\":\"label\", \"v2\":\"message\"})\n",
    "messages.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')\n",
    "#These words and punctuation don’t really say anything about people’ emotions,\n",
    "#so we need to get rid of them if they are in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_processing(mess):\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "#Let’s “tokenise” these messages. \n",
    "#Tokenisation is just the term used to describe the process of \n",
    "#converting the normal text strings in to a list of tokens (words that we actually want).\n",
    "#There are a lot of ways to continue normalising this text such as Stemming or distinguishing by part of speech\n",
    "\n",
    "\n",
    "\n",
    "#Here we have a function named text_processing, and we are processing lines/tweets. \n",
    "#You can name them anything. Ex: bake(pizza). A mess/pizza is a input, and we process/bake it. \n",
    "#At this point, this is just a raw function. \n",
    "#Think of it as y = ax + b. Now we do something with it.\n",
    "\n",
    "#nopunc = [char for char in mess if char not in string.punctuation]\n",
    "#this is a for loop in Python, and it basically says: for character in line, \n",
    "#if character is not in the punctuation list, take it.\n",
    "#So if I have a line: “Hi! My name is Hung “William” Mai, and I am from Vietnam.” \n",
    "#This “sub-function” will take everything that is not a punctation and put them in a list.\n",
    "#So you should have nopunc = [Hi, My, name, is, Hung, William, Mai, and, I, am, from, Vietnam]. \n",
    "#All the punctuations were deleted, and you are left with a list of words\n",
    "\n",
    "#nopunc = ''.join(nopunc)\n",
    "#return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "#Now that we have a list with all the elements separated by a comma, \n",
    "#we would want to put them all again to be processed/joined again \n",
    "#so that it can be processed with another sub-function to get rid of all the unimportant words.\n",
    "#From “Hi My name is Hung William Mai and I am from Vietnam”, we would get \n",
    "#“Hi name Hung William Mai Vietnam”, then a list that has [Hi, name, Hung, William, Mai, Vietnam].\n",
    "#In general, we only take what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    }
   ],
   "source": [
    "messx = messages['message'][0]\n",
    "print (messx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].head(1).apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                          [Ok, lar, Joking, wif, u, oni]\n",
       "2       [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3           [U, dun, say, early, hor, U, c, already, say]\n",
       "4       [Nah, dont, think, goes, usf, lives, around, t...\n",
       "5       [FreeMsg, Hey, darling, 3, weeks, word, back, ...\n",
       "6       [Even, brother, like, speak, treat, like, aids...\n",
       "7       [per, request, Melle, Melle, Oru, Minnaminungi...\n",
       "8       [WINNER, valued, network, customer, selected, ...\n",
       "9       [mobile, 11, months, U, R, entitled, Update, l...\n",
       "10      [Im, gonna, home, soon, dont, want, talk, stuf...\n",
       "11      [SIX, chances, win, CASH, 100, 20000, pounds, ...\n",
       "12      [URGENT, 1, week, FREE, membership, å£100000, ...\n",
       "13      [Ive, searching, right, words, thank, breather...\n",
       "14                                         [DATE, SUNDAY]\n",
       "15      [XXXMobileMovieClub, use, credit, click, WAP, ...\n",
       "16                                    [Oh, kim, watching]\n",
       "17      [Eh, u, remember, 2, spell, name, Yes, v, naug...\n",
       "18      [Fine, thatåÕs, way, u, feel, ThatåÕs, way, go...\n",
       "19      [England, v, Macedonia, dont, miss, goalsteam,...\n",
       "20                               [seriously, spell, name]\n",
       "21         [IÛ÷m, going, try, 2, months, ha, ha, joking]\n",
       "22                 [Ì, pay, first, lar, da, stock, comin]\n",
       "23      [Aft, finish, lunch, go, str, lor, Ard, 3, smt...\n",
       "24               [Ffffffffff, Alright, way, meet, sooner]\n",
       "25      [forced, eat, slice, Im, really, hungry, tho, ...\n",
       "26                              [Lol, always, convincing]\n",
       "27      [catch, bus, frying, egg, make, tea, eating, m...\n",
       "28      [Im, back, amp, packing, car, Ill, let, know, ...\n",
       "29       [Ahhh, Work, vaguely, remember, feel, like, Lol]\n",
       "                              ...                        \n",
       "5542                    [Armand, says, get, ass, epsilon]\n",
       "5543          [U, still, havent, got, urself, jacket, ah]\n",
       "5544    [Im, taking, derek, amp, taylor, walmart, Im, ...\n",
       "5545                          [Hi, durban, still, number]\n",
       "5546                         [Ic, lotta, childporn, cars]\n",
       "5547    [contract, mobile, 11, Mnths, Latest, Motorola...\n",
       "5548                                 [trying, weekend, V]\n",
       "5549    [know, wot, people, wear, shirts, jumpers, hat...\n",
       "5550                             [Cool, time, think, get]\n",
       "5551            [Wen, get, spiritual, deep, Thats, great]\n",
       "5552    [safe, trip, Nigeria, Wish, happiness, soon, c...\n",
       "5553                             [Hahahause, brain, dear]\n",
       "5554    [Well, keep, mind, Ive, got, enough, gas, one,...\n",
       "5555    [Yeh, Indians, nice, Tho, kane, bit, shud, go,...\n",
       "5556          [Yes, thats, u, texted, Pshewmissing, much]\n",
       "5557    [meant, calculation, ltgt, units, ltgt, school...\n",
       "5558                            [Sorry, Ill, call, later]\n",
       "5559         [arent, next, ltgt, hours, imma, flip, shit]\n",
       "5560                        [Anything, lor, Juz, us, lor]\n",
       "5561    [Get, dump, heap, mom, decided, come, lowes, B...\n",
       "5562    [Ok, lor, Sony, ericsson, salesman, ask, shuhu...\n",
       "5563                             [Ard, 6, like, dat, lor]\n",
       "5564        [dont, wait, til, least, wednesday, see, get]\n",
       "5565                                           [Huh, lei]\n",
       "5566    [REMINDER, O2, get, 250, pounds, free, call, c...\n",
       "5567    [2nd, time, tried, 2, contact, u, U, å£750, Po...\n",
       "5568                   [Ì, b, going, esplanade, fr, home]\n",
       "5569                     [Pity, mood, Soany, suggestions]\n",
       "5570    [guy, bitching, acted, like, id, interested, b...\n",
       "5571                                   [Rofl, true, name]\n",
       "Name: message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].head().apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=text_processing).fit(messages['message'])\n",
    "\n",
    "#Now we'll convert each tweets, represented as a list of tokens (lemmas) above, \n",
    "#into a vector that machine learning models can understand.\n",
    "#Doing that requires essentially three steps, in the bag-of-words model:\n",
    "\n",
    "\n",
    "#counting how many times does a word occur in each message (term frequency)\n",
    "#weighting the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "#normalizing the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "#Each vector has as many dimensions as there are unique words in the tweet corpus:\n",
    "\n",
    "#urrently, we have the tweets as lists of tokens (lemmas) \n",
    "#and we need to convert each of those messages into a vector that SciKit Learn’s algorithm models can work with.\n",
    "#There are three steps using the bag-of-words model:\n",
    "#Count how many times does a word occur in each message (Known as term frequency)\n",
    "#Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "#Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "#Each vector will have as many dimensions as there are unique words in the SMS corpus. \n",
    "#We will first use SciKit Learn’s CountVectorizer. \n",
    "#This model will convert a collection of text documents to a matrix of token counts.\n",
    "#We can imagine this as a 2-Dimensional matrix. \n",
    "#Where the 1-dimension is the entire vocabulary (1 row per word) \n",
    "#and the other dimension are the actual documents, in this case a \n",
    "#column per text message. Since there are so many messages, we can \n",
    "#expect a lot of zero counts for the presence of that word in that \n",
    "#document. Because of this, SciKit Learn will output a Sparse Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "mess4 = messages['message'][3]\n",
    "print (mess4)\n",
    "\n",
    "#Here I have used scikit-learn (sklearn), a powerful Python library for teaching machine learning. \n",
    "#It contains a multitude of various methods and options.\n",
    "#Let's take one text message and get its bag-of-words counts as a vector,\n",
    "#putting to use our new bow_transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3996)\t2\n",
      "  (0, 4551)\t1\n",
      "  (0, 5179)\t1\n",
      "  (0, 6118)\t1\n",
      "  (0, 6136)\t1\n",
      "  (0, 7091)\t1\n",
      "  (0, 9445)\t2\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([mess4])\n",
    "print(bow4)\n",
    "# this means that there are 7 unique words in message 4 and two of them appeared twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages_bow =bow_transformer.transform(messages['message'])\n",
    "\n",
    "#Now we can use .transform on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9445)\t0.539151517936\n",
      "  (0, 7091)\t0.438375519235\n",
      "  (0, 6136)\t0.318314301131\n",
      "  (0, 6118)\t0.299155129554\n",
      "  (0, 5179)\t0.296919567515\n",
      "  (0, 4551)\t0.265857766339\n",
      "  (0, 3996)\t0.409247086127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)\n",
    "\n",
    "#after the counting, the term weighting and normalization can be done with TF-IDF, \n",
    "#using scikit-learn's TfidfTransformer\n",
    "\n",
    "#Occurrence count is a good start but there is an issue: \n",
    "#longer documents will have higher average count values than shorter documents, \n",
    "#even though they might talk about the same topics.\n",
    "#To avoid these potential discrepancies it suffices to divide the number of occurrences \n",
    "#of each word in a document by the total number of words in the document: these new features \n",
    "#are called tf for Term Frequencies.\n",
    "#Another refinement on top of tf is to downscale weights for words that occur \n",
    "#in many documents in the corpus and are therefore less informative than those \n",
    "#that occur only in a smaller portion of the corpus.\n",
    "#This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "#After the counting, the term weighting and normalization can be done with TF-IDF, using scikit-learn’s TfidfTransformer.\n",
    "#One of the simplest ranking functions is computed by summing the tf-idf for each query term; \n",
    "#many more sophisticated ranking functions are variants of this simple model.\n",
    "#TF: Term Frequency, which measures how frequently a term occurs in a document. \n",
    "#Since every document is different in length, it is possible that a term would appear \n",
    "#much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length \n",
    "#(aka. the total number of terms in the document) as a way of normalization:\n",
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "#IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, \n",
    "#all terms are considered equally important. However it is known that certain terms, such as \n",
    "#“is”, “of”, and “that”, may appear a lot of times but have little importance. \n",
    "#Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "##now we have the TF and IDF numbers (respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "#To transform the entire bag-of-words corpus into TF-IDF corpus at once:\n",
    "\n",
    "#we firstly use the fit(..) method to fit our estimator to the data \n",
    "#and secondly the transform(..) method to transform our count-matrix to a tf-idf representation. \n",
    "#These two steps can be combined to achieve the same end result faster by skipping redundant processing. \n",
    "#This is done through using the fit_transform(..) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UttamSinha\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#You should never actually evaluate on the same dataset you train on! \n",
    "#Such evaluation tells us nothing about the true predictive power of our model. \n",
    "#If we simply remembered each example during training, the accuracy on training data would trivially be 100%, \n",
    "#even though we wouldn’t be able to classify any new messages.\n",
    "#A proper way is to split the data into a training/test set, where the model only ever sees the training data \n",
    "#during its model fitting and parameter tuning. The test data is never used in any way. \n",
    "#This ensure that our final evaluation on test data is representative of true predictive performance.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msg_train,msg_test,label_train,label_test = train_test_split(messages['message'],messages['label'],test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fitting a Random forest classifier\n",
    "pipeline2 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_processing)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_processing at 0x000000B3DFE4A9D8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preproce...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.fit(msg_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      1.00      0.98      1455\n",
      "       spam       1.00      0.77      0.87       217\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predictions2 = pipeline2.predict(msg_test)\n",
    "print(classification_report(label_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer #Stemmers remove morphological affixes from words, leaving only the word stem\n",
    "from nltk.corpus import stopwords     #\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def pre_process(text):    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))# remove punctuation\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "  \n",
    "\n",
    "# split the text string into individual words, stem each word\n",
    "# and append the stemmed word to words (make sure there's a single\n",
    "# space between each stemmed word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures = messages['message'].copy()\n",
    "textFeatures = textFeatures.apply(pre_process)\n",
    "vectorizer = TfidfVectorizer(\"english\")\n",
    "features = vectorizer.fit_transform(textFeatures)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, messages['label'], test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97846889952153115"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "svc.fit(features_train, labels_train)\n",
    "prediction = svc.predict(features_test)\n",
    "accuracy_score(labels_test,prediction)\n",
    "\n",
    "#svm.SVC documentation states that the \"degree\" parameter is significant for the sigmoid kernel. \n",
    "#Also, it says that \"gamma\" is a coefficient for the polynomial kernel (and not the sigmoid kernel)\n",
    "#However, \"gamma\" is used as a coefficient for the sigmoid kernel, and \"degree\" is not relevant for this kernel. \n",
    "#Also, \"gamma\" is not related to the polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97248803827751196"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='sigmoid', gamma=2.0)\n",
    "svc.fit(features_train, labels_train)\n",
    "prediction = svc.predict(features_test)\n",
    "accuracy_score(labels_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98504784688995217"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "mnb.fit(features_train, labels_train)\n",
    "prediction = mnb.predict(features_test)\n",
    "accuracy_score(labels_test,prediction)\n",
    "\n",
    "#The multinomial Naive Bayes classifier is suitable for classification with discrete features \n",
    "#(e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. \n",
    "#However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=messages.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orpus=[]\n",
    "for i in range(0,5572):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    ps=PorterStemmer()\n",
    "    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    orpus.append(review)\n",
    "    \n",
    "#re.sub(pattern, repl, string[, count, flags])\n",
    "#Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by \n",
    "#the replacement repl. If the pattern isn’t found, string is returned unchanged. repl can be a string or a \n",
    "#function; if it is a string, any backslash escapes in it are processed. That is, \\n is converted to a single newline character, \n",
    "#\\r is converted to a linefeed, and so forth. Unknown escapes such as \\j are left alone. Backreferences, such as \n",
    "#\\6, are replaced with the substring matched by group 6 in the pattern. For example:\n",
    "#>>> re.sub(r'def\\s+([a-zA-Z_][a-zA-Z_0-9]*)\\s*\\(\\s*\\):',\n",
    "#...        r'static PyObject*\\npy_\\1(void)\\n{',\n",
    "#...        'def myfunc():')\n",
    "#'static PyObject*\\npy_myfunc(void)\\n{    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#CountVectorizer implements both tokenization and occurrence counting in a single class\n",
    "cv=CountVectorizer(max_features=3000)\n",
    "x=cv.fit_transform(orpus).toarray()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#LabelEncoder can be used to normalize labels.\n",
    "#It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n",
    "le=LabelEncoder() \n",
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)\n",
    "#consider X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "#consider Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier=GaussianNB()\n",
    "#clf = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "#clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8753363228699551\n",
      "Precision score: 0.54296875\n",
      "Recall score: 0.8633540372670807\n",
      "F1 score: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "pred=classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_test, pred)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, pred)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, pred)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.967713004484305\n",
      "Precision score: 0.9921259842519685\n",
      "Recall score: 0.782608695652174\n",
      "F1 score: 0.8750000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier1=RandomForestClassifier(n_estimators=15,criterion='entropy')\n",
    "#classifier1 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifier1.fit(X_train,y_train)\n",
    "#X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                            n_informative=2, n_redundant=0,\n",
    "#                            random_state=0, shuffle=False)\n",
    "#clf.fit(X, y)\n",
    "predRF=classifier1.predict(X_test)\n",
    "#predict(X)\tPredict class for X.\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_test, predRF)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, predRF)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, predRF)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, predRF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
